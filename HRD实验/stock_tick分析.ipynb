{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as gbt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 量化交易预测v0.1\n",
    "\n",
    "#### 当前版本实现时间\n",
    "2020年8月31日\n",
    "\n",
    "#### StockPredict类读取数据版本\n",
    "20200831 | 20200901\n",
    "\n",
    "#### 主要内容描述\n",
    "首先通过该方法分析能够使用的特征有哪些，原始数据文件中提供的tick数据中有很多特征本身无作用。在该版本对比当前时刻的卖一价和1分钟后的卖一价，如果价格上升则打标签为1，否则打标签为0。使用GBDT模型做初步预测，挑选有价值的特征，并且得到该模型/策略的baseline效果。\n",
    "\n",
    "#### TODO List\n",
    "- 完成单日股票数据可视化接口\n",
    "- 完成决策树绘制接口\n",
    "- 对特征进行转换之后再次尝试模型效果（可以在数据加载之后处理，不破坏原始数据）\n",
    "- 核对当前流程是否有误\n",
    "- 特征&类别标签相关性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import shutil\n",
    "import concurrent.futures as cf\n",
    "import lightgbm as lgb\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(asctime)s - [line:%(lineno)d] - %(levelname)s: %(message)s',\n",
    "                    filename='predict.log',\n",
    "                    filemode='w')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, recall_score, precision_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "class StockDataBasic(object):\n",
    "    def __init__(self, root_path, parall = False):\n",
    "        self.version = \"__version_0828__\"\n",
    "        self.version_desc = \"This is the alpha version 0.1 which just try to make every thing work.\"\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "        self._root_path = root_path\n",
    "        self._stock_list = None\n",
    "        self._root_label_path = None\n",
    "        self._root_data_path = None\n",
    "        self._root_train_path = None\n",
    "        self._parall = parall\n",
    "        \n",
    "        if root_path[-1] == \"/\":\n",
    "            self._root_label_path = root_path + \"label/\"\n",
    "            self._root_data_path = root_path + \"data/\"\n",
    "            self._root_train_path = root_path + \"train/\"\n",
    "        else:\n",
    "            self._root_label_path = root_path + \"/label/\"\n",
    "            self._root_data_path = root_path + \"/data/\"\n",
    "            self._root_train_path = root_path + \"train/\"\n",
    "            \n",
    "        if self._root_data_path:\n",
    "            self._stock_list = os.listdir(self._root_data_path)\n",
    "\n",
    "        \n",
    "    def __load_label(self, stock_file, csv_file):\n",
    "        \"\"\"\n",
    "        load the label of data.\n",
    "        :params stock_file: stock dir name, such as 002001.SZ\n",
    "        :params csv_file: csv file name that contains data.\n",
    "        :return: label result in dictory formation.\n",
    "        \"\"\"\n",
    "        file_path = self._root_label_path + stock_file + \"/\" + csv_file\n",
    "        assert os.path.exists(file_path), 'the given file_path: %s does not esixt.' % file_path\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "        ret_dict = df.set_index(\"MDTime\").T.to_dict('int')\n",
    "        return ret_dict['tag']\n",
    "\n",
    "\n",
    "    def __load_raw_data(self, stock_file, csv_file, white_list = None):\n",
    "        \"\"\"\n",
    "        load the original data that without labels.\n",
    "        :params stock_file: stock dir name, such as 002001.SZ\n",
    "        :params csv_file: csv file name that contains data.\n",
    "        :return: raw data in pandas.DataFrame formation.\n",
    "        \"\"\"\n",
    "        file_path = self._root_data_path + stock_file + \"/\" + csv_file\n",
    "        assert os.path.exists(file_path), 'the given file_path: %s does not esixt.' % file_path\n",
    "        \n",
    "        if white_list == None:\n",
    "            white_list = ['MDTime', 'PreClosePx', 'NumTrades', 'TotalVolumeTrade', 'TotalValueTrade',\n",
    "                       'LastPx', 'OpenPx', 'ClosePx', 'HighPx', 'LowPx', 'MaxPx', 'MinPx', 'DiffPx1', \n",
    "                       'DiffPx2', 'Buy1Price', 'Buy1OrderQty' ,'Sell1Price', 'Sell1OrderQty'\n",
    "            ]\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df[white_list]\n",
    "    \n",
    "        \n",
    "    def data_preprocess(self):\n",
    "        \"\"\"\n",
    "        load the original raw data and its label data to generate the final training/testing data.\n",
    "        warning: this process might be time-consuming!\n",
    "        return: None\n",
    "        \"\"\"\n",
    "        if os.path.exists(self._root_train_path):\n",
    "            shutil.rmtree(self._root_train_path)\n",
    "        os.mkdir(self._root_train_path)\n",
    "        \n",
    "        for stock in self._stock_list:\n",
    "            cur_stock_train_root_path = self._root_train_path + stock\n",
    "            cur_stock_label_root_path = self._root_label_path + stock\n",
    "            if not os.path.exists(cur_stock_train_root_path):\n",
    "                os.mkdir(cur_stock_train_root_path)\n",
    "            \n",
    "            csv_file_list = os.listdir(cur_stock_label_root_path)\n",
    "            csv_file_lent = len(csv_file_list)\n",
    "            \n",
    "            # if process the file in-parall.\n",
    "            if self._parall:\n",
    "                pp = cf.ProcessPoolExecutor()\n",
    "                process_list = list()\n",
    "                for index, csv_file in enumerate(csv_file_list):\n",
    "                    logging.info(\"Processing %d/%d file: %s/%s\" % (index + 1, csv_file_lent, stock, csv_file))\n",
    "                    obj = pp.submit(self.__merge_data_and_label, stock, csv_file)\n",
    "                    process_list.append(obj)\n",
    "                pp.shutdown()\n",
    "            else:\n",
    "                for index, csv_file in enumerate(csv_file_list):\n",
    "                    logging.info(\"Processing %d/%d file: %s/%s\" % (index + 1, csv_file_lent, stock, csv_file))\n",
    "                    self.__merge_data_and_label(stock, csv_file)\n",
    "                    \n",
    "            # WARNING: THE SENTENCE JUST FOR DEBUG, DELETE IT IF YOU WANT TO RUN ALL THE DATA.\n",
    "            # break\n",
    "        \n",
    "    \n",
    "    def __merge_data_and_label(self, stock_name, csv_file):\n",
    "        \"\"\"\n",
    "        Merge the corresponding data and annotated label, the final result will be written into the disk.\n",
    "        The root path was recorded in self._root_train_path.\n",
    "        return: None\n",
    "        \"\"\"\n",
    "        label = self.__load_label(stock_name, csv_file)\n",
    "        data = self.__load_raw_data(stock_name, csv_file)\n",
    "        data['class'] = None\n",
    "        for idx, row in data.iterrows():\n",
    "            if row['MDTime'] in label:\n",
    "                data.loc[idx, 'class'] = label[row['MDTime']]\n",
    "        data.dropna(subset = ['class'], inplace = True)\n",
    "#         del data['MDTime']\n",
    "        data.to_csv(self._root_train_path + stock_name + \"/train_\" + csv_file, index = False)\n",
    "    \n",
    "    \n",
    "class PredictModel(StockDataBasic):\n",
    "    def __init__(self, data_root_path, model_name = \"lightgbm\"):\n",
    "        super(PredictModel, self).__init__(data_root_path)\n",
    "        self.model_name = model_name\n",
    "        self.model_root_path = None\n",
    "        self.result_root_path = None\n",
    "        self.model = None\n",
    "        self.predict_ticks = self.__generate_predict_ticks()\n",
    "        self.parameters = {\n",
    "                'objective': 'binary',\n",
    "                \"boosting\": \"gbdt\",\n",
    "                'max_depth': 5,\n",
    "                'learning_rate': 0.1,\n",
    "                \"feature_fraction\": 0.9,\n",
    "                \"bagging_fraction\": 0.9,\n",
    "                \"nthread\": -1,\n",
    "                'metric': {'binary_logloss'},\n",
    "                \"random_state\": 2020,\n",
    "            }\n",
    "        \n",
    "        if data_root_path[-1] == \"/\":\n",
    "            self.model_root_path = data_root_path + \"model/\"\n",
    "            self.result_root_path = data_root_path + \"result/\"\n",
    "        else:\n",
    "            self.model_root_path = data_root_path + \"/model/\"\n",
    "            self.result_root_path = data_root_path + \"/result/\"\n",
    "    \n",
    "    def __generate_predict_ticks(self):\n",
    "        return_list = list()\n",
    "        for hour in [9, 10, 11, 13, 14]:\n",
    "            for minute in range(60):\n",
    "                for second in [0, 30]:\n",
    "                    if hour == 9 and minute < 31:\n",
    "                        continue\n",
    "                    if hour == 11 and minute >= 30:\n",
    "                        continue\n",
    "                    cur_timestamp = str(hour) + str(minute).zfill(2) + str(second).zfill(2) + '000'\n",
    "                    return_list.append(int(cur_timestamp))\n",
    "        return return_list[:-7]\n",
    "    \n",
    "    \n",
    "    def load_data(self, stock_name, train_test_split = \"20200610\"):\n",
    "        \"\"\"\n",
    "        load the training data, including the train set and test set.\n",
    "        :params stock_name: name of the target stock.\n",
    "        :params train_test_split: split the data set into training set and test set.\n",
    "        return: train set and test test, in pandas.DataFrame formation.\n",
    "        \"\"\"\n",
    "        stock_train_root_path = self._root_train_path + stock_name + '/'\n",
    "        assert os.path.exists(stock_train_root_path), \"the train data dir: %s does not exist.\" % stock_train_root_path\n",
    "        \n",
    "        train_test_split = int(train_test_split)\n",
    "        df_train_positive, df_train_negative, df_test = list(), list(), list()\n",
    "        csv_list = os.listdir(stock_train_root_path)\n",
    "        for csv in csv_list:\n",
    "            try:\n",
    "                csv_file_date = int(csv.split('_')[0])\n",
    "                df_cur = pd.read_csv(stock_train_root_path + csv)\n",
    "                if csv_file_date < train_test_split:\n",
    "                    data = df_cur.values\n",
    "                    for i in range(data.shape[0]):\n",
    "                        if data[i][-1] == 0:\n",
    "                            df_train_negative.append(data[i])\n",
    "                        else:\n",
    "                            df_train_positive.append(data[i])\n",
    "                else:\n",
    "                    data = df_cur.values\n",
    "                    for i in range(data.shape[0]):\n",
    "                        if int(data[i][0]) in self.predict_ticks:\n",
    "                            df_test.append(data[i])\n",
    "            except Exception as e:\n",
    "                logging.error(\"Error while processing csv file: %s\" % stock_train_root_path + csv)\n",
    "                logging.error(e)\n",
    "                continue\n",
    "        \n",
    "        random.shuffle(df_train_negative)\n",
    "        random.shuffle(df_train_positive)\n",
    "#         min_len = min(len(df_train_positive), len(df_train_negative)) + 1\n",
    "#         np_train = np.vstack([df_train_positive[-min_len:], df_train_negative[-min_len:]])\n",
    "        np_train = np.vstack([df_train_positive, df_train_negative])\n",
    "        np.random.shuffle(np_train)\n",
    "        df_train = pd.DataFrame(np_train)\n",
    "        df_test = pd.DataFrame(df_test)\n",
    "        return df_train, df_test\n",
    "    \n",
    "    \n",
    "    def predict_daily(self, stock_name, test_threshold = \"20200610\"):\n",
    "        \"\"\"\n",
    "        Predict the daily result.\n",
    "        :params stock_name: name of stock.\n",
    "        :params test_threshold: only the date after this threshold be seen as test data(the threshold date was included).\n",
    "        return: None.\n",
    "        \"\"\"\n",
    "        stock_train_root_path = self._root_train_path + stock_name + '/'\n",
    "        assert os.path.exists(stock_train_root_path), \"the train data dir: %s does not exist.\" % stock_train_root_path\n",
    "        \n",
    "        test_threshold = int(test_threshold)\n",
    "        csv_list = os.listdir(stock_train_root_path)\n",
    "        for csv in csv_list:\n",
    "            logging.info(\"predict proba for %s/%s\" % (stock_name, csv))\n",
    "            try:\n",
    "                csv_file_date = int(csv.split('_')[0])\n",
    "                if csv_file_date < test_threshold:\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                logging.error(\"Error while processing csv file: %s\" % stock_train_root_path + csv)\n",
    "                logging.error(e)\n",
    "                continue\n",
    "            \n",
    "            df_test = pd.read_csv(stock_train_root_path + csv)\n",
    "            original_matrix = df_test.values\n",
    "            filtered_matrix = list()\n",
    "            for i in range(len(original_matrix)):\n",
    "                if original_matrix[i][0] in self.predict_ticks:\n",
    "                    filtered_matrix.append(original_matrix[i])\n",
    "            \n",
    "            filtered_matrix = np.array(filtered_matrix)\n",
    "            X_test, y_test = filtered_matrix[:, 1:-1], filtered_matrix[:, -1]\n",
    "            y_pred_prob = predict_model.model.predict(X_test, num_iteration = predict_model.model.best_iteration)\n",
    "            \n",
    "            result_df = pd.DataFrame()\n",
    "            result_df['date'] = csv.split('_')[0]\n",
    "            result_df['orderTime'] = self.predict_ticks\n",
    "            result_df['proba'] = y_pred_prob\n",
    "#             result_df['label'] = y_test\n",
    "            result_df['date'] = csv.split('_')[0]\n",
    "            self.save_predict_result(stock_name, csv, result_df)\n",
    "    \n",
    "    \n",
    "    def save_predict_result(self, stock_name, csv_file, df):\n",
    "        \"\"\"\n",
    "        Store the predicted result into csv file.\n",
    "        :params stock_name: name of the stock.\n",
    "        :csv_file: name of the csv file.\n",
    "        return: None.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.result_root_path):\n",
    "            os.mkdir(self.result_root_path)\n",
    "            \n",
    "        stock_dir_path = self.result_root_path + stock_name\n",
    "        if not os.path.exists(stock_dir_path):\n",
    "            os.mkdir(stock_dir_path)\n",
    "        \n",
    "        csv_path = stock_dir_path + '/predict_' + csv_file\n",
    "        df.to_csv(csv_path, index = False)\n",
    "\n",
    "    \n",
    "def load_data_test():\n",
    "    stock_predict_test = StockDataBasic(root_path)\n",
    "\n",
    "    label = stock_predict_test.__load_label(\"002001.SZ\", \"20200525_002001.csv\")\n",
    "    data = stock_predict_test.__load_raw_data(\"002001.SZ\", \"20200525_002001.csv\")\n",
    "    data['class'] = None\n",
    "    for idx, row in data.iterrows():\n",
    "        if row['MDTime'] in label:\n",
    "            data.loc[idx, 'class'] = label[row['MDTime']]\n",
    "    data.dropna(subset = ['class'], inplace = True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def logistic_obj(y_hat, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    p = y_hat\n",
    "    grad = p - y\n",
    "    hess = p * (1. - p)\n",
    "    grad = 4 * p * y + p - 5 * y\n",
    "    hess = (4 * y + 1) * (p * (1.0 - p))\n",
    "    return grad, hess\n",
    "\n",
    "\n",
    "def err_rate(y_hat, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    y_hat = np.clip(y_hat, 10e-7, 1-10e-7)\n",
    "    loss_fn = y * np.log(y_hat)\n",
    "    loss_fp = (1.0 - y) * np.log(1.0 - y_hat)\n",
    "    return 'error', np.sum(-(5 * loss_fn + loss_fp)) / len(y), False\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    ROOT_PATH = \"./stock_predict/\"\n",
    "    \n",
    "#     # test\n",
    "#     test_data = load_data_test(ROOT_PATH, FILE_PATH)\n",
    "#     print(test_data)\n",
    "    \n",
    "#     # generating the training data here.\n",
    "#     stock_predict = StockDataBasic(ROOT_PATH, parall = False)\n",
    "#     stock_predict.data_preprocess()\n",
    "\n",
    "    predict_model = PredictModel(ROOT_PATH)\n",
    "    # train中所有股票代码\n",
    "    stock_list = os.listdir(predict_model._root_train_path)\n",
    "    index = 1\n",
    "    t0 = time.time()\n",
    "    results = []\n",
    "    for stock in stock_list:\n",
    "        # 以每支股票为训练、验证和测试单位\n",
    "        train, test = predict_model.load_data(stock)\n",
    "        if train.shape[0] < 50 or test.shape[0] <= 0:\n",
    "            logging.error(\"train data was not enough for training or test data missed: %s\" % stock)\n",
    "            continue\n",
    "        \n",
    "        # split the train dataset into training set and validation set, ratio 4:1.\n",
    "        # X不取第一列时间戳，y只取class\n",
    "        X_trval, y_trval = train.values[:, 1:-1], train.values[:, -1]\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X_trval, y_trval, test_size = 0.2, random_state = 2020)\n",
    "        # testing set.\n",
    "        X_test, y_test = test.values[:, 1:-1], test.values[:, -1]\n",
    "        \n",
    "        # load training data and evaluation data with lightgbm.Dataset()\n",
    "        train_data = lgb.Dataset(X_train, y_train)\n",
    "        eval_data = lgb.Dataset(X_valid, y_valid)\n",
    "        \n",
    "        predict_model.model = lgb.train(predict_model.parameters, \n",
    "                                        train_data,\n",
    "                                        num_boost_round=50, \n",
    "                                        valid_sets = [train_data, eval_data], \n",
    "                                        early_stopping_rounds = 5,\n",
    "                                        feval = err_rate,\n",
    "#                                         fobj = logistic_obj,\n",
    "                                        verbose_eval=0)\n",
    "\n",
    "        # evaluate the model on testing data.\n",
    "        y_pred_prob = predict_model.model.predict(X_test, num_iteration = predict_model.model.best_iteration)\n",
    "        y_pred = [0 if score <= 0.5 else 1 for score in y_pred_prob]\n",
    "        logging.info(\"predict result on all testset %s: acc = %f, auc = %f\" \\\n",
    "                      % (stock, accuracy_score(y_test, y_pred), roc_auc_score(y_test, y_pred_prob)))\n",
    "        print(\"[%d/%d](%d%s) - %s \\tacc:%f  auc:%f  precision:%f  recall:%f\" \\\n",
    "               % (index, len(stock_list), 100 * index / float(len(stock_list)), '%', stock, \n",
    "               accuracy_score(y_test, y_pred), roc_auc_score(y_test, y_pred_prob),\n",
    "               precision_score(y_test, y_pred), recall_score(y_test, y_pred)))\n",
    "        results.append([stock, accuracy_score(y_test, y_pred), roc_auc_score(y_test, y_pred_prob),\n",
    "               precision_score(y_test, y_pred), recall_score(y_test, y_pred)])\n",
    "        \n",
    "        # predict the probability \n",
    "        predict_model.predict_daily(stock)\n",
    "        index += 1\n",
    "    t1 = time.time()\n",
    "    print(\"all prediction tasks done, time costed: %dm %ds\" % ((t1 - t0) / 60, (t1 - t0) % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"stock\", \"acc\", \"auc\", \"precision\", \"recall\"]\n",
    "df_result = pd.DataFrame(np.array(results), columns = columns)\n",
    "df_result.to_csv(\"./predict_log_0911_2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(root_path):\n",
    "    for path in root_path:\n",
    "        ROOT_PATH = path\n",
    "        \n",
    "        predict_model = PredictModel(ROOT_PATH)\n",
    "        stock_list = os.listdir(predict_model._root_train_path)\n",
    "        for stock in stock_list:\n",
    "            train, test = predict_model.load_data(stock)\n",
    "            \n",
    "            # shuffle the train and test dataset.\n",
    "            # train_shuff = shuffle(train)\n",
    "            # test_shuff  = shuffle(test)\n",
    "\n",
    "            # split the train dataset into training set and validation set, ratio 4:1.\n",
    "            X_trval, y_trval = train.values[:, 1:-1], train.values[:, -1]\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(X_trval, y_trval, test_size = 0.2, random_state = 2020)\n",
    "            # testing set.\n",
    "            X_test, y_test = test.values[:, 1:-1], test.values[:, -1]\n",
    "\n",
    "            print(\"train positive: %d | train negative: %d\" % (y_trval.sum(), len(train) - y_trval.sum()))\n",
    "            print(\"test positive : %d | test negative : %d\" % (y_test.sum(), len(test) - y_test.sum()))\n",
    "\n",
    "            # load training data and evaluation data with lightgbm.Dataset()\n",
    "            train_data = lgb.Dataset(X_train, label = y_train)\n",
    "            eval_data = lgb.Dataset(X_valid, y_valid)\n",
    "            \n",
    "#             for gridsearch\n",
    "#             parameters = {\n",
    "#                 'max_depth': [4, 6, 8],\n",
    "#                 'learning_rate': [0.01, 0.1, 0.5],\n",
    "#                 'num_iteration': [20, 40, 60, 80, 100]\n",
    "#             }\n",
    "            \n",
    "#             predict_model.model = lgb.LGBMClassifier(objective = 'binary',\n",
    "#                                                     metric = 'binary_logloss',\n",
    "#                                                     max_depth = 6,\n",
    "#                                                     learning_rate = 0.1,\n",
    "#                                                     feature_fraction = 0.9,\n",
    "#                                                     bagging_fraction = 0.9)\n",
    "            \n",
    "#             gsearch = GridSearchCV(predict_model.model, param_grid = parameters, scoring = 'accuracy', cv = 3)\n",
    "#             gsearch.fit(X_trval, y_trval)\n",
    "#             print('参数的最佳取值:{0}'.format(gsearch.best_params_))\n",
    "#             break\n",
    "            \n",
    "#             参数的最佳取值:{'num_iteration': 40, 'learning_rate': 0.01, 'max_depth': 4}\n",
    "#             train the lightgbm model\n",
    "            params = {\n",
    "                'objective': 'binary',\n",
    "                \"boosting\": \"gbdt\",\n",
    "                'max_depth': 4,\n",
    "                'num_iteration': 40,\n",
    "                'learning_rate': 0.01,\n",
    "                \"feature_fraction\": 0.9,\n",
    "                \"bagging_fraction\": 0.9,\n",
    "                \"nthread\": -1,\n",
    "                'metric': {'binary_logloss'},\n",
    "                \"random_state\": 2020,\n",
    "            }\n",
    "\n",
    "            bst = lgb.train(params, train_data, num_boost_round=50, valid_sets = [train_data, eval_data], early_stopping_rounds = 5, verbose_eval=0)\n",
    "            bst.save_model('model_test.txt', num_iteration = bst.best_iteration)\n",
    "\n",
    "            # evaluate the model on testing data.\n",
    "            y_pred_prob = bst.predict(X_test, num_iteration = bst.best_iteration)\n",
    "            y_pred = [0 if score < 0.5 else 1 for score in y_pred_prob]\n",
    "            print(\"%s%s acc = %f\" % (path, stock, accuracy_score(y_test, y_pred)))\n",
    "            print(\"%s%s auc = %f\" % (path, stock, roc_auc_score(y_test, y_pred_prob)))\n",
    "            print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尝试使用sklearn自带的XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "ROOT_PATH = \"E:/stock_predict/stock_data/20200831/\"\n",
    "predict_model = PredictModel(ROOT_PATH)\n",
    "train, test = predict_model.load_data(\"002001.SZ\", \"20200610\")\n",
    "\n",
    "# shuffle the train and test dataset.\n",
    "# train_shuff = shuffle(train)\n",
    "# test_shuff  = shuffle(test)\n",
    "\n",
    "# split the train dataset into training set and validation set, ratio 4:1.\n",
    "X_trval, trval_label = train.values[:, 2:-1], train.values[:, -1]\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_trval, trval_label, test_size = 0.2, random_state = 2020)\n",
    "# testing set.\n",
    "X_test, y_test = test.values[:, 2:-1], test.values[:, -1]\n",
    "\n",
    "print(\"train positive: %d | train negative: %d\" % (trval_label.sum(), len(train) - trval_label.sum()))\n",
    "print(\"test positive : %d | test negative : %d\" % (y_test.sum(), len(test) - y_test.sum()))\n",
    "\n",
    "# load training data and evaluation data with lightgbm.Dataset()\n",
    "train_data = lgb.Dataset(X_train, label = y_train)\n",
    "eval_data = lgb.Dataset(X_valid, y_valid, reference=train_data)\n",
    "\n",
    "# train the lightgbm model\n",
    "params = {\n",
    "    'max_depth': 20,\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators': 50,\n",
    "}\n",
    "\n",
    "bst = xgb.XGBClassifier(max_depth = 10,\n",
    "                       learning_rate = 0.01,\n",
    "                       n_estimators = 50,\n",
    "                       objective = 'binary:logistic')\n",
    "bst.fit(X_train. y_train)\n",
    "\n",
    "# evaluate the model on testing data.\n",
    "# y_pred_prob = bst.predict(X_test, num_iteration = bst.best_iteration)\n",
    "# y_pred = [0 if score < 0.5 else 1 for score in y_pred_prob]\n",
    "# print(\"acc = \", accuracy_score(y_test, y_pred))\n",
    "# print(\"auc = \", roc_auc_score(y_test, y_pred_prob))\n",
    "# print(\"confusuion matrix = \", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "\n",
    "iris = load_iris()\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(iris.data, iris.target)\n",
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"iris\") \n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=iris.feature_names,  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "file_path = \"E:/stock_predict/stock_data/20200901/train/002001.SZ/\"\n",
    "csv_list = os.listdir(file_path)\n",
    "df = pd.read_csv(file_path + csv_list[2])\n",
    "df = df[df['MDTime'] > 93000000]\n",
    "df['class'] = df['class'] +25\n",
    "px.line(df, x=\"MDTime\", y=[\"LastPx\", \"PreClosePx\", \"HighPx\", \"LowPx\", \"Sell1Price\", \"Buy1Price\", \"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[df_test[0] > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "from __future__ import print_function, absolute_import, unicode_literals\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from gm.api import *\n",
    "import sys\n",
    "try:\n",
    "    from sklearn import svm\n",
    "except:\n",
    "    print('请安装scikit-learn库和带mkl的numpy')\n",
    "    sys.exit(-1)\n",
    "'''\n",
    "本策略选取了七个特征变量组成了滑动窗口长度为15天的训练集,随后训练了一个二分类(上涨/下跌)的支持向量机模型.\n",
    "若没有仓位则在每个星期一的时候输入标的股票近15个交易日的特征变量进行预测,并在预测结果为上涨的时候购买标的.\n",
    "若已经持有仓位则在盈利大于10%的时候止盈,在星期五损失大于2%的时候止损.\n",
    "特征变量为:1.收盘价/均值2.现量/均量3.最高价/均价4.最低价/均价5.现量6.区间收益率7.区间标准差\n",
    "训练数据为:SHSE.600000浦发银行,时间从2016-03-01到2017-06-30\n",
    "回测时间为:2017-07-01 09:00:00到2017-10-01 09:00:00\n",
    "'''\n",
    "def init(context):\n",
    "    # 订阅浦发银行的分钟bar行情\n",
    "    context.symbol = 'SHSE.600000'\n",
    "    subscribe(symbols=context.symbol, frequency='60s')\n",
    "    start_date = '2016-03-01'  # SVM训练起始时间\n",
    "    end_date = '2017-06-30'  # SVM训练终止时间\n",
    "    # 用于记录工作日\n",
    "    # 获取目标股票的daily历史行情\n",
    "    recent_data = history(context.symbol, frequency='1d', start_time=start_date, end_time=end_date, fill_missing='last',\n",
    "                          df=True)\n",
    "    days_value = recent_data['bob'].values\n",
    "    days_close = recent_data['close'].values\n",
    "    days = []\n",
    "    # 获取行情日期列表\n",
    "    print('准备数据训练SVM')\n",
    "    for i in range(len(days_value)):\n",
    "        days.append(str(days_value[i])[0:10])\n",
    "    x_all = []\n",
    "    y_all = []\n",
    "    for index in range(15, (len(days) - 5)):\n",
    "        # 计算三星期共15个交易日相关数据\n",
    "        start_day = days[index - 15]\n",
    "        end_day = days[index]\n",
    "        data = history(context.symbol, frequency='1d', start_time=start_day, end_time=end_day, fill_missing='last',\n",
    "                       df=True)\n",
    "        close = data['close'].values\n",
    "        max_x = data['high'].values\n",
    "        min_n = data['low'].values\n",
    "        amount = data['amount'].values\n",
    "        volume = []\n",
    "        for i in range(len(close)):\n",
    "            volume_temp = amount[i] / close[i]\n",
    "            volume.append(volume_temp)\n",
    "        close_mean = close[-1] / np.mean(close)  # 收盘价/均值\n",
    "        volume_mean = volume[-1] / np.mean(volume)  # 现量/均量\n",
    "        max_mean = max_x[-1] / np.mean(max_x)  # 最高价/均价\n",
    "        min_mean = min_n[-1] / np.mean(min_n)  # 最低价/均价\n",
    "        vol = volume[-1]  # 现量\n",
    "        return_now = close[-1] / close[0]  # 区间收益率\n",
    "        std = np.std(np.array(close), axis=0)  # 区间标准差\n",
    "        # 将计算出的指标添加到训练集X\n",
    "        # features用于存放因子\n",
    "        features = [close_mean, volume_mean, max_mean, min_mean, vol, return_now, std]\n",
    "        x_all.append(features)\n",
    "    # 准备算法需要用到的数据\n",
    "    for i in range(len(days_close) - 20):\n",
    "        if days_close[i + 20] > days_close[i + 15]:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        y_all.append(label)\n",
    "    x_train = x_all[: -1]\n",
    "    y_train = y_all[: -1]\n",
    "    # 训练SVM\n",
    "    context.clf = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False,\n",
    "                          tol=0.001, cache_size=200, verbose=False, max_iter=-1,\n",
    "                          decision_function_shape='ovr', random_state=None)\n",
    "    context.clf.fit(x_train, y_train)\n",
    "    print('训练完成!')\n",
    "def on_bar(context, bars):\n",
    "    bar = bars[0]\n",
    "    # 获取当前年月日\n",
    "    today = bar.bob.strftime('%Y-%m-%d')\n",
    "    # 获取数据并计算相应的因子\n",
    "    # 于星期一的09:31:00进行操作\n",
    "    # 当前bar的工作日\n",
    "    weekday = datetime.strptime(today, '%Y-%m-%d').isoweekday()\n",
    "    # 获取模型相关的数据\n",
    "    # 获取持仓\n",
    "    position = context.account().position(symbol=context.symbol, side=PositionSide_Long)\n",
    "    # 如果bar是新的星期一且没有仓位则开始预测\n",
    "    if not position and weekday == 1:\n",
    "        # 获取预测用的历史数据\n",
    "        data = history_n(symbol=context.symbol, frequency='1d', end_time=today, count=15,\n",
    "                         fill_missing='last', df=True)\n",
    "        close = data['close'].values\n",
    "        train_max_x = data['high'].values\n",
    "        train_min_n = data['low'].values\n",
    "        train_amount = data['amount'].values\n",
    "        volume = []\n",
    "        for i in range(len(close)):\n",
    "            volume_temp = train_amount[i] / close[i]\n",
    "            volume.append(volume_temp)\n",
    "        close_mean = close[-1] / np.mean(close)\n",
    "        volume_mean = volume[-1] / np.mean(volume)\n",
    "        max_mean = train_max_x[-1] / np.mean(train_max_x)\n",
    "        min_mean = train_min_n[-1] / np.mean(train_min_n)\n",
    "        vol = volume[-1]\n",
    "        return_now = close[-1] / close[0]\n",
    "        std = np.std(np.array(close), axis=0)\n",
    "        # 得到本次输入模型的因子\n",
    "        features = [close_mean, volume_mean, max_mean, min_mean, vol, return_now, std]\n",
    "        features = np.array(features).reshape(1, -1)\n",
    "        prediction = context.clf.predict(features)[0]\n",
    "        # 若预测值为上涨则开仓\n",
    "        if prediction == 1:\n",
    "            # 获取昨收盘价\n",
    "            context.price = close[-1]\n",
    "            # 把浦发银行的仓位调至95%\n",
    "            order_target_percent(symbol=context.symbol, percent=0.95, order_type=OrderType_Market,\n",
    "                                 position_side=PositionSide_Long)\n",
    "            print('SHSE.600000以市价单开多仓到仓位0.95')\n",
    "    # 当涨幅大于10%,平掉所有仓位止盈\n",
    "    elif position and bar.close / context.price >= 1.10:\n",
    "        order_close_all()\n",
    "        print('SHSE.600000以市价单全平多仓止盈')\n",
    "    # 当时间为周五并且跌幅大于2%时,平掉所有仓位止损\n",
    "    elif position and bar.close / context.price < 1.02 and weekday == 5:\n",
    "        order_close_all()\n",
    "        print('SHSE.600000以市价单全平多仓止损')\n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    strategy_id策略ID,由系统生成\n",
    "    filename文件名,请与本文件名保持一致\n",
    "    mode实时模式:MODE_LIVE回测模式:MODE_BACKTEST\n",
    "    token绑定计算机的ID,可在系统设置-密钥管理中生成\n",
    "    backtest_start_time回测开始时间\n",
    "    backtest_end_time回测结束时间\n",
    "    backtest_adjust股票复权方式不复权:ADJUST_NONE前复权:ADJUST_PREV后复权:ADJUST_POST\n",
    "    backtest_initial_cash回测初始资金\n",
    "    backtest_commission_ratio回测佣金比例\n",
    "    backtest_slippage_ratio回测滑点比例\n",
    "    '''\n",
    "    run(strategy_id='strategy_id',\n",
    "        filename='main.py',\n",
    "        mode=MODE_BACKTEST,\n",
    "        token='token_id',\n",
    "        backtest_start_time='2017-07-01 09:00:00',\n",
    "        backtest_end_time='2017-10-01 09:00:00',\n",
    "        backtest_adjust=ADJUST_PREV,\n",
    "        backtest_initial_cash=10000000,\n",
    "        backtest_commission_ratio=0.0001,\n",
    "        backtest_slippage_ratio=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "iris = load_iris()\n",
    "data = iris.data\n",
    "target = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2)\n",
    " \n",
    "# 创建成lgb特征的数据集格式\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    " \n",
    "# 将参数写成字典下形式\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',  # 设置提升类型\n",
    "    'objective': 'regression',  # 目标函数\n",
    "    'metric': {'l2', 'auc'},  # 评估函数\n",
    "    'num_leaves': 31,  # 叶子节点数\n",
    "    'learning_rate': 0.05,  # 学习速率\n",
    "    'feature_fraction': 0.9,  # 建树的特征选择比例\n",
    "    'bagging_fraction': 0.8,  # 建树的样本采样比例\n",
    "    'bagging_freq': 5,  # k 意味着每 k 次迭代执行bagging\n",
    "    'verbose': 1  # <0 显示致命的, =0 显示错误 (警告), >0 显示信息\n",
    "}\n",
    " \n",
    "# 训练 cv and train\n",
    "gbm = lgb.train(params, lgb_train, num_boost_round=20, valid_sets=lgb_eval, early_stopping_rounds=5)\n",
    " \n",
    "# 保存模型到文件\n",
    "gbm.save_model('model.txt')\n",
    " \n",
    "# 预测数据集\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    " \n",
    "# 评估模型\n",
    "print('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}